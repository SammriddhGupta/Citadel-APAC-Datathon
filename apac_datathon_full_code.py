# -*- coding: utf-8 -*-
"""APAC_Datathon_Full_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zCa3FDVkThjO2dNAa6zOopRsmQZuY5nT

# Data Wrangling

## Setting up environment
"""

from google.colab import drive
import numpy as np 
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt

drive.mount('/content/drive')

"""To run this make sure you have created a shortcut in Drive linking to the APAC Datasets 2023 folder

###Importing datasets and formating column names to lower case
"""

crash_general = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_general.csv')
crash = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_vehicles.csv') # crash variable := crash vehicles dataset initially
crash_motorcycle = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_motorcycle.csv')
crash_commercial = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_commericial_vehicles.csv')
crash_trail = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_trailed_vehicles.csv')
crash_people = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_people.csv')
crash_roadway = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_roadway.csv')
crash_flag = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/Crashes/crash_info_flag_variables.csv')
ds = [crash_general, crash_trail, crash_commercial, crash, crash_motorcycle, crash_flag, crash_people, crash_roadway]
for i in ds:
    i.columns = i.columns.str.lower()

"""###Filtering useful columns"""

crash = crash[['crn',
               'dvr_pres_ind', #Alcohol or Drugs involvement
               'grade', 
               'make_cd',
               'rdwy_alignment',
               'travel_spd',    
               'veh_type',
               'veh_movement',
               'veh_role',
               'impact_point',
               'owner_driver',
                'unit_num',
                'unit_type',
               'veh_position']]
crash_people = crash_people[crash_people.person_type == 1] #we only want to analyze the driver
crash = crash[crash.veh_role !=2] #Remove vehicles not responsible for accident 
crash_general.rename(columns={'dec_lat': 'lat'}, inplace=True)
crash_general.rename(columns={'dec_long': 'lng'}, inplace=True)
crash_general_essentials = [
                'crn',
                'collision_type',
                'crash_year',
                'crash_month',
                'time_of_day',
                'fips',
                'lat',
                'lng',
                'illumination',
                'hour_of_day',
                'location_type',
                'intersect_type',
                'max_severity_level',
                #'fatal_count',
                #'tot_inj_count',
                'weather1',
                'weather2',
                'road_condition',
                'relation_to_road',
                'rdwy_surf_type_cd',
]
crash_general = crash_general[crash_general_essentials]
print(f"Number of vehicles responsible for crahes = {crash.shape[0]}")

"""###Additional cleaning 


*   Replaced 999 speeds with the 40th quanilte of their respective vehicle type, say 50 for buses etc
*   Replaced nan and empty entires in sex with U
*   Replaced nan values in illumination with 9 (unknown)




"""

crash_people.sex = crash_people.sex.replace([np.nan, ' '], 'U')
crash_general.rdwy_surf_type_cd = crash_general.rdwy_surf_type_cd.replace(np.nan, 9) 
crash_general.illumination = crash_general.illumination.replace(np.nan, 9)
#Replace values in travel_spd above 75th quartile with median value
for i in range(1, 9):
    crash.loc[(crash.veh_type == i) & (crash.travel_spd > crash[crash.veh_type == i].travel_spd.quantile(0.4)), 'travel_spd'] = crash[crash.veh_type == i].travel_spd.quantile(0.4)

"""###Merging Age Data 
Includes Age, Sex and Injury Level

*   Somehow there are more entries after merging case, which doesnt happen with the other datasets, im guessing there are two possible reasons, either 1) there are duplicates in the people dataset, or 2) there are more than one driver in some vehicles
*   Only drivers age are merged (person_type = 1)
*   Switched 99yo drivers to the mean age of the brand of their car (I assume older people can afford to buy more expensive brands)
*   Though by removing ppl < 16 yo, the number of vehicles is more closer to the original amount



"""

#Assign age to crash based on unit_num in crash_people
crash = crash.merge(crash_people[['crn','unit_num','age','sex','inj_severity']], on=['crn','unit_num'], how='left')
crash.age.value_counts()  
crash.head()

"""Replacing extreme age values"""

crash.loc[crash.age == 99, 'age'] = crash.groupby('make_cd')['age'].transform('mean').round(0)
crash = crash[crash.age > crash.age.quantile(0.001)] 
crash.age.value_counts()

"""###Merging crash_general


*   Roadway info
*   Weather
*   Fatality and Injury count
*   Fips, Lat and Lng
*   Year, month, hour and minute


"""

crash = pd.merge(crash, crash_general[crash_general_essentials], on='crn', how='left')
#crash.travel_spd = crash.travel_spd.mask(crash.travel_spd > crash.travel_spd.quantile(0.75), crash.groupby('veh_type').travel_spd.transform('median'))
crash.shape[0]

"""###Stitching Road Type Data 
- (Though coming last since if it is the root cause it wouldve been resolved by the government already)
"""

crash = pd.merge(crash, crash_roadway[['speed_limit','crn']], on='crn', how='left')
crash.head()

"""###Classifying special vehicle types to merge with crash_commerical, crash_trail and crash_motorcycle later on"""

dataset_to_special = {
    'crash_commercial': 'commercial',
    'crash_motorcycle': 'motorcycle',
    'crash_trail': 'trail'  
}

# Iterate over the datasets and assign values to the 'special' column
for dataset_name, special_value in dataset_to_special.items():
    dataset = globals()[dataset_name]
    mask = crash['crn'].isin(dataset['crn'])
    crash.loc[mask, 'special'] = special_value

"""Merging crash_commercial"""

crash_commercial = crash_commercial[['crn',
                                    'unit_num',
                                    'osize_load_ind',
                                    'gvwr',
                                    'cargo_bd_type',
                                    ]]

#Merge commercial with crash_commercial, ensuring that the shape of the resulting dataframe is the same as crash_commercial
crash_commercial = pd.merge(crash_commercial, crash, on=['crn','unit_num'], how='left')
crash_commercial.shape[0]

#Drop duplicates of crn and unit_num
crash_commercial = crash_commercial.drop_duplicates(subset=['crn','unit_num'], keep='first')
crash_commercial.shape[0]

"""Merging crash.motorcycle"""

#Merge crash_motorcycle with crash, ensuring that the shape of the resulting dataframe is the same as crash_motorcycle
crash_motorcycle = crash_motorcycle[['crn','unit_num']]
crash_motorcycle = pd.merge(crash_motorcycle, crash, on=['crn','unit_num'], how='left')
crash_motorcycle = crash_motorcycle[crash_motorcycle.special == 'motorcycle']
crash_motorcycle.shape[0]
crash_motorcycle = crash_motorcycle[crash_motorcycle.veh_type != np.nan]

"""Merge crash_flag with crash_general for testing

*   Added lat and lng so we could group flag variables in regions


"""

crash_flag = crash_flag.merge(crash_general[['crn','fips','lat','lng']], on='crn', how='left')

#common = crash.columns.intersection(crime.columns)
#crash = pd.merge(crash, crime[['dispatch_date']], on=common, how='left')
#crash.dispatch_date.head()

"""##Generating regions
These regions will be used to analyse crash frequency by location.

### Exploring location data

The minimum and maximum values for location data, in addition to the datatypes for the attributes, indicate that all values which are not null are valid. LEss than 0.5% of entries are missing a given geospatial data attribute, indicating these attributes can be used in analysis. However it should be noted that patterns in missing data may make the dataset not representative of Philadelphian crashes, impacting generalisability of findings. Furthermore, the FIPS data spanned multiple counties with no obvious valid aggregation method, making the latitude and longitude data more suitable for analysis.
"""

print(crash.columns)

# Looking for unexpected values
print(crash[['fips', 'lat', 'lng']].min())
print(crash[['fips', 'lat', 'lng']].max())

# Looking for null values
print(crash[['fips', 'lat', 'lng']].isna().sum())
print(crash[['fips', 'lat', 'lng']].isna().sum() / len(crash) * 100)
print(len(crash['fips'].unique()))

"""### Transforming geospatial data

Philadelphia has an area of ~370km^2, which is 19.2km**2 OR Using google maps, we determined the dimensions of Philadelphia as ~28 x 28km. One degree of latitude is ~111km, and at 40 degrees N one degree of longitude is ~85km. This means our range encompasses about 126km by 117km. To have appropriately granular investigation of spatial distribution over all data but also inside the city centre, we determine bins of size total latitude or longitude/10 are suitable, producing 100 total regions of sizw ~12.6km x 11.7km.
"""

# Finding range of values
print(crash[['lat', 'lng']].min()) 
print(crash[['lat', 'lng']].max())
print(crash[['lat', 'lng']].max() - crash[['lat', 'lng']].min())

# Determining regions
lat_bins = 10
lng_bins = 10
crash['lat_region'] = pd.cut(crash['lat'], lat_bins, labels=range(0,lat_bins))
crash['lng_region'] = pd.cut(crash['lng'], lng_bins, labels=range(0,lng_bins))
crash['region'] = crash['lat_region'].astype(str) + ", " + crash["lng_region"].astype(str)
print(crash[['lat_region', 'lng_region', 'region']])

# Checking output
print(crash['region'].value_counts())
# This indicated there is one key outlier charecterised as the only record in the minimum latitude region and maximum longitude

"""After calculating regions, it was apparent that one outlier data point was significantly increasing the range of the location data, and this was removed to allow for more appropiate analysis of crashes within philadelphia. Using the same process as before, we find a range of ~ 33 x 43 km, and our 100 regions are each ~ 3.3 x 4.3 km. We then remove any record without enough location data to determine a region."""

# Removing sole outlier
print(crash[crash['lat_region'] < 2])
crash.drop(crash.loc[crash['crn'] == 2020015307].index, inplace=True)

# Recalculating range of data
print(crash[['lat', 'lng']].min()) 
print(crash[['lat', 'lng']].max())
print(crash[['lat', 'lng']].max() - crash[['lat', 'lng']].min())

# Determining regions
crash.drop(['lat_region', 'lng_region', 'region'], axis=1, inplace=True)
crash['lat_region'] = pd.cut(crash['lat'], lat_bins, labels=range(0,lat_bins))
crash['lng_region'] = pd.cut(crash['lng'], lng_bins, labels=range(0,lng_bins))
crash['region'] = crash['lat_region'].astype(str) + ", " + crash["lng_region"].astype(str)
print(crash[['lat_region', 'lng_region', 'region']])
#crash['region'].mask(crash['region'].str.contains('nan'), 'nan', inplace=True)
crash.drop(crash.loc[crash['region'].str.contains('nan')].index, inplace=True)

# Checking output
print(crash['region'].value_counts())

"""##Exporting wrangled csv"""

print(len(crash['crn'].unique()))
print(len(crash))
crash.to_csv('/content/drive/MyDrive/APAC_2023_Datasets/processed/cleaned_crash.csv', index=False)

"""#Generating counts by factors

Using Pandas Group By to simplify counting. Note that NaN values are skipped as a value for a factor.
"""

crash_region_counts = {"Factor": [], "Value": [], "Region": [], "Count": []}
''' Sample dataset
All, True, 1, 20
All, True, 2, 30
Drunk Driver, True, 1, 1
Drunk Driver, True, 2, 30
Drunk Driver, False, 1, 19
Drunk Driver, False, 2, 0
'''

total_counts = crash[['crn', 'region']].groupby(['region']).count()
for row in total_counts.index:
    crash_region_counts["Factor"].append('All')
    crash_region_counts["Value"].append('')
    crash_region_counts["Region"].append(total_counts.loc[row].name)
    crash_region_counts["Count"].append(total_counts.loc[row]['crn'])

factors = ['dvr_pres_ind', 'grade','rdwy_alignment',
       'travel_spd', 'veh_type', 'veh_movement', 'veh_role', 'impact_point',
       'owner_driver', 'unit_type', 'veh_position', 'age', 'sex',
       'inj_severity', 'collision_type',  'illumination',
       'hour_of_day', 'location_type', 'intersect_type', 'max_severity_level',
       'weather1', 'weather2', 'road_condition', 'relation_to_road', 
       'rdwy_surf_type_cd', 'speed_limit', 'special']

for attribute in factors:
  attribute_df = crash[['crn','region', attribute]]
  # THIS SKIPS NA VALUES FOR AN ATTRIBUTE (e.g. dvr_pres_ind = nan) -> counts.loc[nan] is invalid
  # downstream analysis on counts must focus on only valid entries for attributes
  counts = attribute_df.groupby([attribute, 'region']).count()
  for row in counts.index:
    # print(counts.loc[row])
    crash_region_counts["Factor"].append(attribute)
    crash_region_counts["Value"].append(counts.loc[row].name[0])
    crash_region_counts["Region"].append(counts.loc[row].name[1])
    crash_region_counts["Count"].append(counts.loc[row]['crn'])
  
crash_region_count_df = pd.DataFrame(crash_region_counts)
print(crash_region_count_df.head(10))
print(len(crash_region_count_df.index))

"""Ensuring every set of regions has counts for every region (even if that count is 0)."""

# make list of all regions
regions = []
for i in range(0, 10):
  for j in range(0, 10):
    regions.append(f"{i}, {j}")

#regions = crash_region_count_df[crash_region_count_df['Factor'] == "All"]['Region']

# making full dataframe with empty counts for every region 
# O(n) where n is rows in final dataset
full_crash_region_counts = {"Factor": [], "Value": [], "Region": [], "Count": []}

factors = crash_region_count_df['Factor'].unique()
for factor in factors:
  values = crash_region_count_df[crash_region_count_df['Factor'] == factor]['Value'].unique()
  for value in values:
    for region in regions:
      full_crash_region_counts['Factor'].append(factor)
      full_crash_region_counts['Value'].append(value)
      full_crash_region_counts['Region'].append(region)
      full_crash_region_counts['Count'].append(0)

full_crash_region_count_df = pd.DataFrame(full_crash_region_counts)

# check new dataframe creation
print(crash_region_count_df[['Factor', 'Value']].drop_duplicates())
print(len(full_crash_region_count_df.index) / len(regions)) # expect this to be same as rows in above

# go through full counts data, and overwrite '0' count if a count is present for the region in old data
old_data_index = 0

for index, row in full_crash_region_count_df.iterrows():
  # if we have filled every value from the old data, terminate loop
  if (old_data_index == len(crash_region_count_df.index)):
    break

  old_row = crash_region_count_df.iloc[old_data_index]
  if(old_row[0] == row[0] and old_row[1] == row[1] and old_row[2] == row[2]):
    full_crash_region_count_df.loc[index, "Count"] = old_row[3]
    old_data_index += 1

# checking final dataframe
print(full_crash_region_count_df)
print(full_crash_region_count_df[full_crash_region_count_df['Count'] != 0])

full_crash_region_count_df.to_csv('/content/drive/MyDrive/APAC_2023_Datasets/processed/counts_by_factor_by_region.csv', index=False)

"""# Statistical Testing on Crash Factors

##Chi square test
"""

from scipy.stats import chisquare

# some variables used are defined when generating regions
results = full_crash_region_count_df
n_regions = len(regions)

total_counts = []
valid_region_indexes = []

cur_counts = []
cur_factor_value = []

insignificant_factors = []
significant_factors = []
invalid_region_count = 0 
invalid_regions_threshold = 0.2
dropped_factor_values = []


# iterating over sets of regions, comparing distributions against total counts
# data frame is sorted by region within each set of regions, and has no missing indices 
for index, row in results.iterrows():
  # set up totals
  if (index < n_regions):
    # need to meet requirements for valid chi-squared test
    if (row[3] >= 5):
      valid_region_indexes.append(index)
      total_counts.append(row[3])

  # after rows for totals
  else:
    # if we are at the start of a set of regions, reset count and store comparison info
    if (index % n_regions == 0):
      cur_factor_value = [row[0], row[1]]
      invalid_region_count = 0
      cur_counts = []

    if (index % n_regions in valid_region_indexes):
      cur_counts.append(row[3])

    invalid_region_count += 1 if (row[3] < 5) else 0
      
    # if we are at the end of a set of regions, carry out statisical testing
    if ((index + 1) % n_regions == 0):
      # we dont want to discount factors for having no crashes when no crashes in the dataset happened at that region
      invalid_region_count -= (n_regions - len(valid_region_indexes))

      if (invalid_region_count < n_regions * invalid_regions_threshold):
        normalised_counts = [count/sum(cur_counts) * sum(total_counts) for count in cur_counts]
        chisq, p = chisquare(normalised_counts, total_counts)
        if (p < 0.05):
          significant_factors.append(cur_factor_value)
        else:
          insignificant_factors.append(cur_factor_value)
      else:
        dropped_factor_values.append(cur_factor_value)

"""## Retrieving test results"""

print("Factors insignificant against total crash distribution at p = 0.05: ")
print(insignificant_factors)
print("Factors significant against total crash distribution at p = 0.05: ")
print(significant_factors)
print("Factors with < 5 crashes for too many regions: ")
print(dropped_factor_values)

"""Categorising factors found as significant for some values as features for predictive model"""

#Categorize significant_factors into factors
model_factors = []
for factor in significant_factors:
    model_factors.append(factor[0])
    #categories.append(factor[1])
model_factors = pd.DataFrame(model_factors, columns = ['Factor'])
model_features = model_factors.Factor.unique()
model_features

print(len(crash.columns))
print(len(model_features))

"""# Visualization

### Seaborn Heatmap Set Up

"""

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

def generate_heatmap_data(factor, value):
  subset = full_crash_region_count_df.loc[(full_crash_region_count_df['Factor'] == factor) & (full_crash_region_count_df['Value'] == value), ["Region", "Count"]]
  subset[["Latitude Region", "Longitude Region"]] = subset["Region"].str.split(", ", expand=True).astype('int')

  pivot_table = pd.pivot_table(subset, values="Count", index="Longitude Region", columns="Latitude Region", aggfunc="sum")
  pivot_table.sort_index(level=0, ascending=False, inplace=True)

  return pivot_table

"""### All Crashes Distribution"""

pivot_table = generate_heatmap_data("All", "")
fig, ax = plt.subplots()
fig.suptitle("Spatial Distribution of All Crashes in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""### Substance use or Sickness indications in Driver"""

#dvr_pres_ind = 3 for illegal drug use and 4 for sickness

pivot_table = generate_heatmap_data("dvr_pres_ind", 3)
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Illegal Drug Use in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("dvr_pres_ind", 4)
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Sick Drivers in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""## Roadway Alignment"""

#RDWY_ALIGNMENT= 1 - Straight; 2 – Curved; 3 – Curve Left; 4 – Curve Right
pivot_table = generate_heatmap_data("rdwy_alignment", 1)
fig, ax = plt.subplots()
fig.suptitle("Spatial Distribution of Crashes Involving Straight Roads in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("rdwy_alignment", 2) + generate_heatmap_data("rdwy_alignment", 3) + generate_heatmap_data("rdwy_alignment", 4)
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Curved Roads in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""## Young Adults"""

pivot_table = generate_heatmap_data("age", 16.0) + generate_heatmap_data("age", 17.0) + generate_heatmap_data("age", 18.0) + generate_heatmap_data("age", 19.0) + generate_heatmap_data("age", 20.0) + generate_heatmap_data("age", 21.0)
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Ages 21 and Under in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""## Illumination"""

#illumination 2 – Dark – no streetlights 3 – Dark – streetlights
pivot_table = generate_heatmap_data("illumination", 2)
fig, ax = plt.subplots()
fig.suptitle("Crashes At Night With No Streetlights in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("illumination", 3)
fig, ax = plt.subplots()
fig.suptitle("Crashes At Night With Streetlights in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""## Weather"""

#weather1 02 - Blowing Snow 04 - Cloudy 05 - Fog, Smog, Smoke 06 - Freezing Rain or Freezing Drizzle 07 - Rain 10 - Snow (10 only in weather1)
pivot_table = generate_heatmap_data("weather1", 6) + generate_heatmap_data("weather2", 6) + generate_heatmap_data("weather1", 7) + generate_heatmap_data("weather2", 7)
fig, ax = plt.subplots()
fig.suptitle("Crashes In Rainy Weather in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("weather1", 2) + generate_heatmap_data("weather2", 2) + generate_heatmap_data("weather1", 5) + generate_heatmap_data("weather2", 5)
fig, ax = plt.subplots()
fig.suptitle("Crashes In Poor Visibility in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("weather1", 2) + generate_heatmap_data("weather2", 2) + generate_heatmap_data("weather1", 10)
fig, ax = plt.subplots()
fig.suptitle("Crashes In Snowy Weather in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""## Special Vehicle Types"""

pivot_table = generate_heatmap_data("special", "commercial") 
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Commercial Vehicles in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("special", "motorcycle") 
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Motorcycles in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

pivot_table = generate_heatmap_data("special", "trail") 
fig, ax = plt.subplots()
fig.suptitle("Crashes Involving Trailers in Philadelphia (2010-2021)")
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='d', linewidths=0.5, ax=ax)
plt.show()

"""#Predictive Model

##Random Forest
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE

data = pd.read_csv('/content/drive/MyDrive/APAC_2023_Datasets/processed/cleaned_crash.csv')
cat = ['make_cd', 'sex','special'] #any categorical feature 
for col in cat:
    data[col] = data[col].astype('category')
    data[col] = data[col].cat.codes

#replace na values with 0
data = data.fillna(0)

cat = ['make_cd', 'sex','special','region']
data.travel_spd = data.travel_spd.fillna(0)
#data = data.drop(columns=('weather2'))
for col in cat:
    data[col] = data[col].astype('category')
    data[col] = data[col].cat.codes

#remove na values
data = data.dropna(0)

features = model_features
target = ['lat','lng']

random_seed = 42
# splitting data into train and test sets
train_dataset, test_dataset, train_labels, test_labels = train_test_split(data[features], data[target], test_size=0.1, random_state=random_seed)

# adding RFE to model
model = RandomForestRegressor(n_estimators=30, random_state=random_seed)

rfe = RFE(model, n_features_to_select=3)

# merging pipeline with RFE and model 
pipeline = Pipeline([
    ('rfe', rfe),
    ('model', model)
])

# fitting pipeline to train data
pipeline.fit(train_dataset, train_labels)

#evaluation
test_predictions = pipeline.predict(test_dataset)
mse = mean_squared_error(test_labels, test_predictions)
mae = mean_absolute_error(test_labels, test_predictions)

print(f'Test MSE: {mse:.2f}')
print(f'Test MAE: {mae:.2f}')

#cross validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, data[features], data[target], cv=5, scoring='neg_mean_squared_error')

mse = -scores.mean()

print(f'Cross-validation MSE: {mse:.2f}')

#plotting results

plt.figure(figsize=(10, 10))
plt.xlabel('True Latitude')
plt.ylabel('Fitted Latitude')
plt.title('1000 True vs Fitted Latitudes')
sc = plt.scatter(test_labels.lat, test_predictions[:, 0], c=test_predictions[:, 0], cmap='viridis', alpha=0.5)
plt.colorbar(sc)
plt.legend()
plt.show()

plt.figure(figsize=(10,10))
plt.scatter(test_labels.lng, test_predictions[:,1], alpha=0.5)
plt.xlabel('True Longitudes')
plt.ylabel('Fitted Longitudes')
plt.title('1000 True vs Fitted Longitudes')
plt.savefig('/content/drive/MyDrive/APAC_2023_Datasets/visualizations/lng.png')
plt.show()

plt.figure(figsize=(10, 10))
plt.scatter(test_labels.lat, test_labels.lng, c='yellow', label='Actual')
plt.scatter(test_predictions[:, 0], test_predictions[:, 1], label='Predicted',alpha=0.1)
#reduce alpha of points to see overlapping points
plt.legend()
plt.show()

from scipy.stats import gaussian_kde

#density plot of predictions
x = test_predictions[:, 0]
y = test_predictions[:, 1]
xy = np.vstack([x, y])
z = gaussian_kde(xy)(xy)

#plot heatmap
fig, ax = plt.subplots()
sc = ax.scatter(x, y, c=z, cmap='viridis', alpha=0.5)
ax.set_xlabel('Predicted Latitude')
ax.set_ylabel('Predicted Longitude')
ax.set_title('Heatmap of predictions')
cbar = fig.colorbar(sc)
cbar.ax.set_ylabel('Intensity')
plt.show()

#error dist
errors = test_predictions - test_labels
plt.hist(errors, bins=20)
plt.xlabel('Prediction error')
plt.ylabel('Frequency')
plt.legend()
plt.title('Prediction error distribution')
plt.show()

# get feature names and importances
feature_names = data[features].columns
feature_importances = pipeline.named_steps['model'].feature_importances_

# sort features by importance
sorted_idx = feature_importances.argsort()
sorted_names = feature_names[sorted_idx]
sorted_importances = feature_importances[sorted_idx]

# plot feature importance bar chart
plt.figure(figsize=(10,6))
sns.barplot(x=sorted_importances, y=sorted_names)
plt.title('Feature Importance of Random Forest Model')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

pred = pd.DataFrame(test_predictions, columns=['lat', 'lng'])

"""Getting the region of predictions for visualization"""

def generating_regions(df, lat_bins=10, lng_bins=10):
    df_min = df[['lat', 'lng']].min()
    df_max = df[['lat', 'lng']].max()
    df_range = df_max - df_min

    df['lat_region'] = pd.cut(df['lat'], lat_bins, labels=range(0,lat_bins))
    df['lng_region'] = pd.cut(df['lng'], lng_bins, labels=range(0,lng_bins))
    df['region'] = df['lat_region'].astype(str) + ", " + df["lng_region"].astype(str)

    #df.drop(df.loc[df['crn'] == 2020015307].index, inplace=True)

    df_min = df[['lat', 'lng']].min()
    df_max = df[['lat', 'lng']].max()
    df_range = df_max - df_min

    df.drop(['lat_region', 'lng_region', 'region'], axis=1, inplace=True)
    df['lat_region'] = pd.cut(df['lat'], lat_bins, labels=range(0,lat_bins))
    df['lng_region'] = pd.cut(df['lng'], lng_bins, labels=range(0,lng_bins))
    df['region'] = df['lat_region'].astype(str) + ", " + df["lng_region"].astype(str)
    
    df.drop(df.loc[df['region'].str.contains('nan')].index, inplace=True)
    
    return df
pred = generating_regions(pred)
test = generating_regions(test_labels)
pred.head()

pred.region = pred.region.astype("category")
test_labels.region = test_labels.region.astype('category')
pred.region = pred.region.cat.codes
test_labels.region = test_labels.region.cat.codes



"""Matching predicted with actual region"""

plt.figure(figsize=(5, 5))
plt.xlabel('Actual Regions')
plt.ylabel('Predicted Regions')
plt.scatter(test_labels.region, pred.region, c='b',alpha =0.3)
plt.legend()
plt.show()